# Awesome-Video-Datasets

## Action Recognition

* **HMDB**: A Large Video Database for Human Motion Recognition (ICCV 2011)</br> 
[[Paper](https://serre-lab.clps.brown.edu/wp-content/uploads/2012/08/Kuehne_etal_iccv11.pdf)][[Homepage](https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/)]

* **UCF101**: A Dataset of 101 Human Actions Classes From Videos in The Wild </br> 
[[Paper](https://www.crcv.ucf.edu/papers/UCF101_CRCV-TR-12-01.pdf)][[Homepage](https://www.crcv.ucf.edu/data/UCF101.php)]

* **ActivityNet**: A Large-Scale Video Benchmark for Human Activity Understanding (CVPR 2015)</br> 
[[Paper](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Heilbron_ActivityNet_A_Large-Scale_2015_CVPR_paper.pdf)][[Homepage](http://activity-net.org/index.html)]

* **Kinetics** </br>  
[[Kinetics-400](https://arxiv.org/abs/1705.06950)] [[Kinetics-600](https://arxiv.org/abs/1808.01340)] [[Kinetics-700](https://arxiv.org/abs/1907.06987)] [[Homepage](https://deepmind.com/research/open-source/kinetics)]

* **Charades**: Hollywood in Homes: Crowdsourcing Data
Collection for Activity Understanding (ECCV 2016)</br> 
[[Paper](http://ai2-website.s3.amazonaws.com/publications/hollywood-homes.pdf)][[Homepage](https://prior.allenai.org/projects/charades)]

* **Charades-Ego**: Actor and Observer: Joint Modeling of First and Third-Person Videos (CVPR 2018) </br> 
[[Paper](https://arxiv.org/pdf/1804.09627.pdf)][[Homepage](https://prior.allenai.org/projects/charades-ego)]

* **Moments in Time Dataset**: one million videos for event understanding (TPAMI 2019) </br> 
[[Paper](http://moments.csail.mit.edu/TPAMI.2019.2901464.pdf)][[Homepage](http://moments.csail.mit.edu/)]

* **Multi-Moments in Time**: Learning and Interpreting Models for Multi-Action Video Understanding </br> 
[[Paper](https://arxiv.org/pdf/1911.00232.pdf)][[Homepage](http://moments.csail.mit.edu/)]

* **20BN-SOMETHING-SOMETHING**: The "something something" video database for learning and evaluating visual common sense </br> 
[[Paper](https://arxiv.org/abs/1706.04261)][[Homepage](https://20bn.com/datasets/something-something)]

* **EPIC-KITCHENS**: Scaling Egocentric Vision: The EPIC-KITCHENS Dataset</br> 
[[Paper](https://openaccess.thecvf.com/content_ECCV_2018/papers/Dima_Damen_Scaling_Egocentric_Vision_ECCV_2018_paper.pdf)][[Homepage](https://epic-kitchens.github.io/2021)]

* **HOMAGE**: Home Action Genome: Cooperative Compositional Action Understanding (CVPR 2021) </br> 
[[Paper]()][[Homepage](https://homeactiongenome.org/)]</br> 
*27 participants, 12 sensor types, 75 activities, 453 atomic actions, 1,752 synchronized sequences, 86 object classes, 29 relationship classes, 497,534 bounding boxes, 583,481 relationships.*

* **MMAct**: A Large-Scale Dataset for Cross Modal Human Action Understanding (ICCV 2019) </br> 
[[Paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Kong_MMAct_A_Large-Scale_Dataset_for_Cross_Modal_Human_Action_Understanding_ICCV_2019_paper.pdf)][[Homepage](https://mmact19.github.io/2019/)]

* **LEMMA**: A Multi-view Dataset for LEarning Multi-agent Multi-task Activities (ECCV 2020) </br> 
[[Paper](https://arxiv.org/pdf/2007.15781.pdf)][[Homepage](https://sites.google.com/view/lemma-activity)]

* **Action Genome**: Actions as Compositions of Spatio-temporal Scene Graphs (CVPR 2020) </br> 
[[Paper](https://arxiv.org/pdf/1912.06992.pdf)][[Homepage](https://www.actiongenome.org/)]

* **TITAN**: Future Forecast using Action Priors (CVPR 2020) </br> 
[[Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Malla_TITAN_Future_Forecast_Using_Action_Priors_CVPR_2020_paper.pdf)][[Homepage](https://usa.honda-ri.com/titan)]

* **PKU-MMD**: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding (ACM Multimedia Workshop) </br> 
[[Paper](https://arxiv.org/abs/1703.07475)][[Homepage](https://github.com/ECHO960/PKU-MMD#pku-mmd-a-large-scale-benchmark-for-continuous-multi-modal-human-action-understanding)]

* **Breakfast**: The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities </br> 
[[Paper](https://openaccess.thecvf.com/content_cvpr_2014/papers/Kuehne_The_Language_of_2014_CVPR_paper.pdf)][[Homepage](https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/)]

## Action/Event Localization
* **AVA**: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions </br> 
[[Paper](https://arxiv.org/abs/1705.08421)][[Homepage](http://research.google.com/ava/)]
* **EEV**: A Large-Scale Dataset for Studying Evoked Expressions from Video </br> 
[[Paper](https://arxiv.org/abs/2001.05488)][[Homepage](https://github.com/google-research-datasets/eev)]
* **MUSES**: Multi-shot Temporal Event Localization: a Benchmark (CVPR 2021) </br> 
[[Paper](https://arxiv.org/pdf/2012.09434.pdf)][[Homepage](https://songbai.site/muses/)]

## Audiovisual Learning
* **AVE**: Audio-Visual Event Localization in Unconstrained Videos </br> 
[[Paper](https://openaccess.thecvf.com/content_ECCV_2018/papers/Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper.pdf)][[Homepage](https://sites.google.com/view/audiovisualresearch)]

* **LLP**: Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video Parsing </br> 
[[Paper](https://arxiv.org/pdf/2007.10558.pdf)][[Homepage](https://github.com/YapengTian/AVVP-ECCV20)]

* **VGG-Sound**: A large scale audio-visual dataset </br> 
[[Paper](https://arxiv.org/abs/2004.14368)][[Homepage](https://www.robots.ox.ac.uk/~vgg/data/vggsound/)]

* **VGG-SS**: Localizing Visual Sounds the Hard Way (CVPR 2021)</br> 
[[Paper](https://arxiv.org/pdf/2104.02691.pdf)][[Homepage](https://www.robots.ox.ac.uk/~vgg/research/lvs/)]
