# Awesome-Video-Datasets

* Contributions are most welcome, if you have any suggestions or improvements, please create an issue or raise a pull request. 
* Our group website: **[VIS Lab](https://ivi.fnwi.uva.nl/vislab/)**, University of Amsterdam. 

## Contents
 - [Action Recognition](ActionRecognition.md)
 - [Video Classification](VideoClassification.md)
 - [Egocentric View](EgocentricView.md)
 - [Video Object Segmentation](VideoObjectSegmentation.md)
 - [Object Detection](ObjectDetection.md)
 - [Group Activity Recognition](GroupActivityRecognition.md)
 - [Movie](Movie.md)
 - [Video Captioning](VideoCaptioning.md)
 - [360 Videos](360Videos.md)
 - [Activity Localization](ActivityLocalization.md)
 - [Video and Language](VideoandLanguage.md)
 - [Video Question Answering](VideoQuestionAnswering.md)
 - [Action Segmentation](#Action-Segmentation)
 - [Repetition Counting](#Repetition-Counting)
 - [Audiovisual Learning](#Audiovisual-Learning)
 - [Video Indexing](#Video-Indexing)
 - [Skill Determination](#Skill-Determination)
 - [Video Retrieval](#Video-Retrieval)
 - [Single Object Tracking](#Single-Object-Tracking)
 - [Multiple Objects Tracking](#Multiple-Objects-Tracking)
 - [Video Relation Detection](#Video-Relation-Detection)
 - [Anomaly Detection](#Anomaly-Detection)
 - [Video Memorability](#Video-Memorability)
 - [Highlight Detection](#Highlight-Detection)
 - [Pose Estimation](#Pose-Estimation)
 - [Animal Behavior](#Animal-Behavior)
 - [Person Re-identification](#Person-Re-Identification)
 - [Dynamic Texture Classification](DynamicTextureClassification.md)
 - [Physics](#Physics)


## Repetition Counting
* **QUVA Repetition**: Real-World Repetition Estimation by Div, Grad and Curl (CVPR 2018) </br>
[[Paper](https://openaccess.thecvf.com/content_cvpr_2018/papers/Runia_Real-World_Repetition_Estimation_CVPR_2018_paper.pdf)][[Homepage](http://tomrunia.github.io/projects/repetition/)] </br>
*100 videos*

* **YTSegments**: Live Repetition Counting (ICCV 2015) </br>
[[Paper](https://openaccess.thecvf.com/content_iccv_2015/papers/Levy_Live_Repetition_Counting_ICCV_2015_paper.pdf)][[Homepage](https://github.com/ofirlevy/repcount)]</br>
*100 videos*

* **UCFRep**: Context-Aware and Scale-Insensitive Temporal Repetition Counting (CVPR 2020) </br>
[[Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Context-Aware_and_Scale-Insensitive_Temporal_Repetition_Counting_CVPR_2020_paper.pdf)][[Homepage](https://github.com/Xiaodomgdomg/Deep-Temporal-Repetition-Counting)] </br>
*526 videos*

* **Countix**: Counting Out Time: Class Agnostic Video Repetition Counting in the Wild (CVPR 2020) </br>
[[Paper](https://arxiv.org/pdf/2006.15418v1.pdf)][[Homepage](https://sites.google.com/view/repnet)] </br>
*8,757 videos*

* **Countix-AV & Extreme Countix-AV**: Repetitive Activity Counting by Sight and Sound (CVPR 2021) </br>
[[Paper](https://arxiv.org/pdf/2103.13096.pdf)][[Homepage](https://github.com/xiaobai1217/RepetitionCounting)] </br>
*1,863 videos in Countix-AV, 214 videos in Extreme Countix-AV*

## Video Indexing
* **MediaMill**: The Challenge Problem for Automated Detection of 101 Semantic Concepts in Multimedia </br>
[[Paper](https://isis-data.science.uva.nl/cgmsnoek/pub/snoek-challenge-acm2006.pdf)][[Homepage](https://ivi.fnwi.uva.nl/isis/mediamill/challenge/)] </br>
*manually annotated concept lexicon*

## Skill Determination

* **EPIC-Skills**: Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination (CVPR 2018) </br>
[[Paper](https://arxiv.org/abs/1703.09913)][[Homepage](https://github.com/hazeld/EPIC-Skills2018)] </br>
*3 tasks, 113 videos, 1000 pairwise ranking annotations*

* **BEST**: The Pros and Cons: Rank-aware Temporal Attention for Skill Determination in Long Videos (CVPR 2019) </br>
[[Paper](https://arxiv.org/abs/1812.05538)][[Homepage](https://github.com/hazeld/rank-aware-attention-network)] </br>
*5 tasks, 500 videos, 13000 pairwise ranking annotations*

* **AQA-7**: Action Quality Assessment Across Multiple Actions (WACV 2019) </br>
[[Paper](https://arxiv.org/pdf/1812.06367)][[Homepage](http://rtis.oit.unlv.edu/datasets.html)] </br>
*1,189 samples from 7 sports: 370 from single diving - 10m platform, 176 from gymnastic vault, 175 from big air skiing, 206 from big air snowboarding, 88 from synchronous diving - 3m springboard, 91 from synchronous diving - 10m platform and 83 from trampoline*

* **AQA-MTL**: What and how well you performed? A multitask approach to action quality assessment (CVPR 2019) </br>
[[Paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Parmar_What_and_How_Well_You_Performed_A_Multitask_Learning_Approach_CVPR_2019_paper.pdf)][[Homepage](http://rtis.oit.unlv.edu/datasets.html)] </br>
*1,412 fine-grained samples collected from 16 different events with various views*

* **Olympic Scoring Dataset**: Learning to score Olympic events (CVPR 2017 Workshop) </br>
[[Paper](https://arxiv.org/abs/1611.05125v3)][[Homepage](http://rtis.oit.unlv.edu/datasets.html)] </br>
*The existing MIT diving dataset is doubled from 159 samples to 370 examples. A new gymnastic vault dataset consisting of 176 samples has been collected*

* **JIGSAWS**: JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS): A Surgical Activity Dataset for Human Motion Modeling </br>
[[Paper](https://cirl.lcsr.jhu.edu/wp-content/uploads/2015/11/JIGSAWS.pdf)][[Homepage](https://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release/)] </br>
*surgical activities, 3 tasks: “Suturing (S)”, ”Needle Passing (NP)” and “Knot Tying (KT)”, each video is annotated with multiple annotation scores assessing different aspects of a video*

## Video Retrieval

* **TRECVID Challenge**: TREC Video Retrieval Evaluation </br>
[[Homepage](https://trecvid.nist.gov/)] </br>
*sources: YFCC100M, Flickr, etc*

* **Video Browser Showdown** – The Video Retrieval Competition </br>
[[Homepage](https://videobrowsershowdown.org/)]

* **TRECVID-VTT**: TRECVID 2019: An Evaluation Campaign to Benchmark Video Activity Detection, Video Captioning and Matching, and Video Search & Retrieval </br>
[[Paper](https://arxiv.org/abs/2009.09984)][[Homepage](https://ir.nist.gov/tv_vtt_data/)] </br>
*9185 videos with captions*

* **V3C** - A Research Video Collection </br>
[[Paper](https://link.springer.com/content/pdf/10.1007%2F978-3-030-05710-7_29.pdf)][[Homepage](https://sites.google.com/view/viral2019/home/supported-datasets)] </br>
*7475 Vimeo videos, 1,082,657 short video segments*

* **IACC**: Creating a web-scale video collection for research </br>
[[Paper](https://dl.acm.org/doi/pdf/10.1145/1631135.1631141)][[Homepage](https://sites.google.com/view/viral2019/home/supported-datasets)] </br>
*4600 Internet Archive videos*

* **TVR**: A Large-Scale Dataset for Video-Subtitle Moment Retrieval (ECCV 2020) </br>
[[Paper](https://arxiv.org/abs/2001.09099)][[Homepage](https://tvr.cs.unc.edu/)] </br>
*108,965 queries on 21,793 videos from 6 TV shows of diverse genres, where each query is associated with a tight temporal alignment*

## Single Object Tracking
* **Lingual OTB99 & Lingual ImageNet Videos**: Tracking by Natural Language Specification (CVPR 2017) </br>
[[Paper](https://isis-data.science.uva.nl/cgmsnoek/pub/li-tracking-language-cvpr2017.pdf)][[Homepage](https://github.com/QUVA-Lab/lang-tracker)] </br>
*natural language descriptions of the target object*

* **OxUvA**: Long-term Tracking in the Wild: A Benchmark (ECCV 2018) </br>
[[Paper](https://arxiv.org/pdf/1803.09502.pdf)][[Homepage](https://oxuva.github.io/long-term-tracking-benchmark/)] </br>
*366 sequences spanning 14 hours of video*

* **LaSOT**: A High-quality Benchmark for Large-scale Single Object Tracking </br>
[[Paper](https://arxiv.org/pdf/1809.07845.pdf)][[Homepage](https://cis.temple.edu/lasot/)] </br>
*1,400 sequences with more than 3.5M frames, each frame is annotated with a bounding box*

* **TNL2K**: Towards More Flexible and Accurate Object Tracking with Natural Language:
Algorithms and Benchmark (CVPR 2021)</br>
[[Paper](https://arxiv.org/pdf/2103.16746.pdf)][[Homepage](https://sites.google.com/view/langtrackbenchmark/)] </br>
*2k video sequences (contains a total of 1,244,340 frames, 663 words) and split 1300/700 for the train/testing respectively; densely annotate one sentence in English and corresponding bounding boxes of the target object for each video.*

* **TrackingNet**: A Large-Scale Dataset and Benchmark for Object Tracking in the Wild (ECCV 2018) </br>
[[Paper](https://arxiv.org/pdf/1803.10794.pdf)][[Homepage](https://tracking-net.org/)] </br>
*30K videos with more than 14 million dense bounding box annotations, a new benchmark composed of 500 novel videos*

* **ALOV300+**: Visual Tracking: An Experimental Survey (TPAMI 2014) </br>
[[Paper](https://ivi.fnwi.uva.nl/isis/publications/2014/SmeuldersTPAMI2014/SmeuldersTPAMI2014.pdf)][[Homepage](https://ivi.fnwi.uva.nl/isis/publications/bibtexbrowser.php?key=SmeuldersTPAMI2014&bib=all.bib)][[Dataset](http://alov300pp.joomlafree.it/)]</br>
*315 videos*

* **NUS-PRO**: A New Visual Tracking Challenge (TPAMI 2015) </br>
[[Paper](https://faculty.ucmerced.edu/mhyang/papers/pami15_nus_pro.pdf)][[Homepage](https://sites.google.com/site/li00annan/nus-pro)] </br>
*365 image sequences*

* **UAV123**: A Benchmark and Simulator for UAV Tracking (ECCV 2016) </br>
[[Paper](https://drive.google.com/file/d/1YDlCOWxH8HMPlmblJ2OvxFdSFecuiM3Z/edit)][[Homepage](https://cemse.kaust.edu.sa/ivul/uav123)] </br>
*123 new and fully annotated HD video sequences captured from a low-altitude aerial perspective*

* **OTB2013**: Online Object Tracking: A Benchmark (CVPR 2013) </br>
[[Paper](https://faculty.ucmerced.edu/mhyang/papers/cvpr13_benchmark.pdf)][[Homepage](http://cvlab.hanyang.ac.kr/tracker_benchmark/benchmark_v10.html)] </br>
*50 video sequences*

* **OTB2015**: Object Tracking Benchmark (TPAMI 2015) </br>
[[Paper](https://ieeexplore.ieee.org/document/7001050)][[Homepage](http://cvlab.hanyang.ac.kr/tracker_benchmark/index.html)] </br>
*100 video sequences*

* **VOT Challenge** </br>
[[Homepage](https://www.votchallenge.net/)]

## Multiple Objects Tracking
* **MOT Challenge** </br>
[[Homepage](https://motchallenge.net/)] </br>

* **VisDrone**: Vision Meets Drones: A Challenge </br>
[[Paper](https://arxiv.org/pdf/1804.07437.pdf)][[Homepage](https://github.com/VisDrone)] </br>

* **TAO**: A Large-Scale Benchmark for Tracking Any Object </br>
[[Paper](https://arxiv.org/abs/2005.10356)][[Homepage](http://taodataset.org/)] </br>
*2,907 videos, 833 classes, 17,287 tracks*

* **GMOT-40**: A Benchmark for Generic Multiple Object Tracking </br>
[[Paper](https://arxiv.org/pdf/2011.11858.pdf)][[Homepage](https://spritea.github.io/GMOT40/)] </br>
*40 carefully annotated sequences evenly distributed among 10 object categories*

* **BDD100K**: A Diverse Driving Dataset for Heterogeneous Multitask Learning (CVPR 2020) </br>
[[Paper](https://arxiv.org/pdf/1805.04687.pdf)][[Homepage](https://www.bdd100k.com/)] </br>
*100K videos and 10 tasks*

* **DroneCrowd**: Detection, Tracking, and Counting Meets Drones in Crowds: A Benchmark (CVPR 2021) </br>
[[Paper](https://arxiv.org/pdf/2105.02440.pdf)][[Homepage](https://github.com/VisDrone/DroneCrowd)] </br>
*112 video clips with 33,600 HD frames in various scenarios, 20,800 people trajectories with 4.8 million heads and several video-level attributes*

## Video Relation Detection
* **KIEV**: Interactivity Proposals for Surveillance Videos </br>
[[Paper](https://isis-data.science.uva.nl/cgmsnoek/pub/chen-interactivity-icmr2020.pdf)][[Homepage](https://github.com/shanshuo/Interactivity_Proposals)] </br>
*a new task of spatio-temporal interactivity proposals*

* **ImageNet-VidVRD**: Video Visual Relation Detection </br>
[[Paper](https://dl.acm.org/doi/10.1145/3123266.3123380)][[Homepage](https://xdshang.github.io/docs/imagenet-vidvrd.html)]</br>
*1,000 videos, 35 common subject/object categories and 132 relationships*

* **VidOR**: Annotating Objects and Relations in User-Generated Videos </br>
[[Paper](https://dl.acm.org/doi/10.1145/3323873.3325056)][[Homepage](https://xdshang.github.io/docs/vidor.html)] </br>
*10,000 videos selected from YFCC100M collection, 80 object categories and 50 predicate categories*

* **Something-Else**: Compositional Action Recognition with Spatial-Temporal Interaction Networks (CVPR 2020) </br>
[[Paper](https://arxiv.org/pdf/1912.09930.pdf)][[Homepage](https://github.com/joaanna/something_else)] </br>
*annotations for 180049 videos from the Something-Something Dataset*

* **Action Genome**: Actions as Compositions of Spatio-temporal Scene Graphs (CVPR 2020) </br> 
[[Paper](https://arxiv.org/pdf/1912.06992.pdf)][[Homepage](https://www.actiongenome.org/)]</br>
*10K videos, 0.4M objects, 1.7M visual relationships*

* **VidSitu**: Visual Semantic Role Labeling for Video Understanding (CVPR 2021) </br>
[[Paper](https://arxiv.org/pdf/2104.00990.pdf)][[Homepage](https://vidsitu.org/)] </br>
*29K 10-second movie clips richly annotated with a verb and semantic-roles every 2 seconds*

## Anomaly Detection
* **XD-Violence**: Not only Look, but also Listen: Learning Multimodal Violence Detection under Weak Supervision (ECCV 2020) </br>
[[Paper](https://roc-ng.github.io/XD-Violence/images/paper.pdf)][[Homepage](https://roc-ng.github.io/XD-Violence/)] </br>
*4,754 untrimmed videos*

* **UCF-Crime**: Real-world Anomaly Detection in Surveillance Videos </br>
[[Paper](https://arxiv.org/pdf/1801.04264.pdf)][[Homepage](https://www.crcv.ucf.edu/projects/real-world/#:~:text=We%20construct%20a%20new%20large,Stealing%2C%20Shoplifting%2C%20and%20Vandalism.)]</br>
*1,900 videos*

## Highlight Detection
* **YouTube Highlights**: Ranking Domain-specific Highlights by Analyzing Edited Videos</br>
[[Paper](http://grail.cs.washington.edu/wp-content/uploads/2015/08/sun2014rdh.pdf)][[Homepage](https://github.com/aliensunmin/DomainSpecificHighlight)] </br>
*six domain-specific categories: surfing, skating, skiing, gymnastics, parkour, and dog. Each domain consists of around 100 videos and the total accumulated time is 1430 minutes*

* **PHD<sup>2</sup>**: Personalized Highlight Detection for Automatic GIF Creation </br>
[[Paper](https://arxiv.org/abs/1804.06604)][[Homepage](https://github.com/gifs/personalized-highlights-dataset)] </br>
*the training set contains highlights from 12,972 users, the test set contains highlights from 850 users*

* **TVSum**: Summarizing web videos using titles (CVPR 2015) </br>
[[Paper](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf)][[Homepage](https://github.com/yalesong/tvsum)] </br>
*50 videos of various genres (e.g., news, how-to, documentary, vlog, egocentric) and 1,000 annotations of shot-level importance scores obtained via crowdsourcing (20 per video)*

## Video Memorability

* **Video Memorability**: Multimodal Memorability: Modeling Effects of Semantics and Decay on Video Memorability (ECCV 2020) </br>
[[Paper](https://arxiv.org/pdf/2009.02568.pdf)][[Homepage](http://memento.csail.mit.edu/)] </br>
*10,000 video clips, over 900,000 human memory annotations at different delay intervals and 50,000 captions describing the events*

## Pose Estimation
* **YouTube Pose**: Personalizing Human Video Pose Estimation (CVPR 2016) </br>
[[Paper](https://arxiv.org/pdf/1511.06676.pdf)][[Homepage](https://www.robots.ox.ac.uk/~vgg/data/pose/index.html)] </br>
*50 videos, 5,000 annotated frames*

* **JHMDB**: Towards understanding action recognition (ICCV 2013) </br>
[[Paper](http://jhmdb.is.tue.mpg.de/show_file?filename=JHMDB_ICCV_2013_corrected.pdf)][[Homepage](http://jhmdb.is.tue.mpg.de/)] </br>
*5,100 clips of 51 different human actions collected from movies or the Internet, 31,838 annotated frames in total*

* **Penn Action**: From Actemes to Action: A Strongly-supervised Representation for Detailed Action Understanding (ICCV 2013) </br>
[[Paper](https://openaccess.thecvf.com/content_iccv_2013/papers/Zhang_From_Actemes_to_2013_ICCV_paper.pdf)][[Homepage](http://dreamdragon.github.io/PennAction/)] </br>
*2,326 video sequences of 15 different actions and human joint annotations for each sequence*

* **PoseCNN**: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes (RSS 2018) </br>
[[Paper](https://arxiv.org/abs/1711.00199)][[Homepage](https://rse-lab.cs.washington.edu/projects/posecnn/)] </br>
*accurate 6D poses of 21 objects from the YCB dataset observed in 92 videos with 133,827 frames*

## Animal Behavior
* **Edinburgh Pig Behavior**: Extracting Accurate Long-Term Behavior Changes from a Large Pig Dataset </br>
[[Paper](https://homepages.inf.ed.ac.uk/rbf/PAPERS/pigsbehaviouranalysis_visapp2021.pdf)][[Homepage](https://homepages.inf.ed.ac.uk/rbf/PIGDATA/)] </br>
*23 days (over 6 weeks) of daytime pig video captured from a nearly overhead camera, 6 frames per second, and stored in batches of 1800 frames (5 minutes), most frames show 8 pigs*

## Person Re-identification
* **iLIDS-VID**: Person re-identification by video ranking (ECCV 2014) </br>
[[Paper](https://core.ac.uk/download/pdf/208789091.pdf)][[Homepage](https://xiatian-zhu.github.io/downloads_qmul_iLIDS-VID_ReID_dataset.html)] </br>
*600 image sequences of 300 distinct individuals*

* **PRID-2011**: Person Re-identification by Descriptive and Discriminative Classification </br>
[[Paper](https://link.springer.com/content/pdf/10.1007%2F978-3-642-21227-7_9.pdf)][[Homepage](https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/prid11/)]</br>
*400 image sequences for 200 identities from two non-overlapping cameras*

* **MARS**:  A Video Benchmark for Large-Scale Person Re-Identification (ECCV 2016) </br>
[[Paper](https://link.springer.com/content/pdf/10.1007%2F978-3-319-46466-4_52.pdf)][[Homepage](http://zheng-lab.cecs.anu.edu.au/Project/project_mars.html)] </br>
*1,261 identities around 18,000 video sequences*

## Physics
* **Real-world Flag & FlagSim**: Cloth in the Wind: A Case Study of Physical Measurement through Simulation (CVPR 2020) </br>
[[Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Runia_Cloth_in_the_Wind_A_Case_Study_of_Physical_Measurement_CVPR_2020_paper.pdf)][[Homepage](http://tomrunia.github.io/projects/cloth/)] </br>
*Real-world Flag: 2.7K train and 1.3K videos clips, FlagSim: 1,000 mesh sequences, 14, 000 training examples*

* **Physics 101**: Learning Physical Object Properties from Unlabeled Videos (BMVC 2016) </br>
[[Paper](http://phys101.csail.mit.edu/papers/phys101_bmvc.pdf)][[Homepage](http://phys101.csail.mit.edu/)] </br>
*over 10,000 video clips containing 101 objects of various materials and appearances (shapes, colors, and sizes)*

* **CLEVRER**: Collision Events for Video Representation and Reasoning (ICLR 2020) </br>
[[Paper](https://arxiv.org/pdf/1910.01442.pdf)][[Homepage](http://clevrer.csail.mit.edu/)] </br>
*10,000 videos for training, 5,000 for validation, and 5,000 for testing; all videos last for 5 seconds; the videos are generated by a physics engine that simulates object motion plus a graphs engine that renders the frames*
