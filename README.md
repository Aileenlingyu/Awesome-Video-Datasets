# Awesome-Video-Datasets

## Action Recognition

* **HOLLYWOOD2**: Actions in Context (CVPR 2009) </br>
[[Paper](http://www.irisa.fr/vista/Papers/2009_cvpr_marszalek.pdf)][[Homepage](https://www.di.ens.fr/~laptev/actions/hollywood2/)]</br>
*12 classes of human actions, 10 classes of scenes, 3,669 clips, 69 movies*

* **HMDB**: A Large Video Database for Human Motion Recognition (ICCV 2011)</br> 
[[Paper](https://serre-lab.clps.brown.edu/wp-content/uploads/2012/08/Kuehne_etal_iccv11.pdf)][[Homepage](https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/)]</br>
*51 classes, 7,000 clips*

* **UCF101**: A Dataset of 101 Human Actions Classes From Videos in The Wild </br> 
[[Paper](https://www.crcv.ucf.edu/papers/UCF101_CRCV-TR-12-01.pdf)][[Homepage](https://www.crcv.ucf.edu/data/UCF101.php)]</br> 
*101 classes, 13k clips*

* **Sports-1M**: Large-scale Video Classification with Convolutional Neural Networks </br>
[[Paper](https://cs.stanford.edu/people/karpathy/deepvideo/deepvideo_cvpr2014.pdf)][[Homepage](https://cs.stanford.edu/people/karpathy/deepvideo/)]</br>
*1,000,000 videos, 487 classes*

* **MPII-MD**: A Dataset for Movie Description </br>
[[Paper](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Rohrbach_A_Dataset_for_2015_CVPR_paper.pdf)][[Homepage](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/vision-and-language/mpii-movie-description-dataset/)] </br>
*94 videos, 68,337 clips, 68,375 descriptions*

* **ActivityNet**: A Large-Scale Video Benchmark for Human Activity Understanding (CVPR 2015)</br> 
[[Paper](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Heilbron_ActivityNet_A_Large-Scale_2015_CVPR_paper.pdf)][[Homepage](http://activity-net.org/index.html)]</br>
*203 classes, 137 untrimmed videos per class, 1.41 activity instances per video*

* **MovieQA**: Story Understanding Benchmark (CVPR 2016) </br>
[[Paper](http://movieqa.cs.toronto.edu/static/files/CVPR2016_MovieQA.pdf)][[Homepage](http://movieqa.cs.toronto.edu/home/#)]</br>
*14,944 questions, 408 movies*

* **DALY**: Human Action Localization with Sparse Spatial Supervision </br>
[[Paper](https://arxiv.org/pdf/1605.05197.pdf)][[Homepage](http://thoth.inrialpes.fr/daly/index.php)] </br>
*10 actions, 3.3M frames, 8,133 clips*

* **MPII-Cooking**: Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data (IJCV 2015) </br>
[[Paper](https://link.springer.com/content/pdf/10.1007/s11263-015-0851-8.pdf)][[Homepage](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-2-dataset/)] </br>
*67 fine-grained activities, 59 composite activities, 14,105 clips, 273 videos*

* **Kinetics** </br>
[[Kinetics-400](https://arxiv.org/abs/1705.06950)/[Kinetics-600](https://arxiv.org/abs/1808.01340)/[Kinetics-700](https://arxiv.org/abs/1907.06987)/[Kinetics-700-2020](https://arxiv.org/pdf/2010.10864.pdf)] [[Homepage](https://deepmind.com/research/open-source/kinetics)]</br>
*400/600/700/700 classes, at least 400/600/600/700 clips per class*

* **Youtube-8M**: A Large-Scale Video Classification Benchmark </br>
[[Paper](https://arxiv.org/pdf/1609.08675.pdf)][[Homepage](https://research.google.com/youtube8m/index.html)]</br>
*8,000,000 videos, 4000 visual entities*

* **Charades**: Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding (ECCV 2016)</br> 
[[Paper](http://ai2-website.s3.amazonaws.com/publications/hollywood-homes.pdf)][[Homepage](https://prior.allenai.org/projects/charades)]</br>
*9,848 annotated videos, 267 people, 27,847 video descriptions, 66,500 temporally localized intervals for 157 action classes and
41,104 labels for 46 object classes*

* **ActivityNet Captions**: Dense-Captioning Events in Videos (ICCV 2017) </br>
[[Paper](https://arxiv.org/pdf/1705.00754.pdf)][[Homepage](https://cs.stanford.edu/people/ranjaykrishna/densevid/)] </br>
*20k videos, 100k sentences*

* **YouTube-BoundingBoxes**: A Large High-Precision Human-Annotated Data Set for Object Detection in Video </br>
[[Paper](https://arxiv.org/pdf/1702.00824.pdf)][[Homepage](https://research.google.com/youtube-bb/index.html)] </br>
*380,000 video segments about 19s long, 5.6 M bounding boxes, 23 types of objects*

* **Charades-Ego**: Actor and Observer: Joint Modeling of First and Third-Person Videos (CVPR 2018) </br> 
[[Paper](https://arxiv.org/pdf/1804.09627.pdf)][[Homepage](https://prior.allenai.org/projects/charades-ego)]</br>
*112 people, 4000 paired videos, 157 action classes*

* **20BN-jester**: The Jester Dataset: A Large-Scale Video Dataset of Human Gestures (ICCVW 2019) </br>
[[Paper](https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Materzynska_The_Jester_Dataset_A_Large-Scale_Video_Dataset_of_Human_Gestures_ICCVW_2019_paper.pdf)][[Homepage](https://20bn.com/datasets/jester)] </br>
*148,092 videos, 27 classes, 1376 actors*

* **Moments in Time Dataset**: one million videos for event understanding (TPAMI 2019) </br> 
[[Paper](http://moments.csail.mit.edu/TPAMI.2019.2901464.pdf)][[Homepage](http://moments.csail.mit.edu/)]</br>
*over 1,000,000 labelled videos for 339 Moment classes, the average number of labeled videos per class is 1,757 with a median of 2,775*

* **Multi-Moments in Time**: Learning and Interpreting Models for Multi-Action Video Understanding </br> 
[[Paper](https://arxiv.org/pdf/1911.00232.pdf)][[Homepage](http://moments.csail.mit.edu/)]</br>
*1.02 million videos, 313 action classes, 553,535 videos are annotated with more than one label and 257,491 videos are annotated with three or more labels*

* **20BN-SOMETHING-SOMETHING**: The "something something" video database for learning and evaluating visual common sense </br> 
[[Paper](https://arxiv.org/abs/1706.04261)][[Homepage](https://20bn.com/datasets/something-something)]</br>
*100,000 videos across 174 classes*

* **DAVIS**: A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation (CVPR 2016) </br>
[[Paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Perazzi_A_Benchmark_Dataset_CVPR_2016_paper.pdf)][[Homepage](https://davischallenge.org/)]</br>
*50 sequences, 3455 annotated frames*

* **YouTube Pose**: Personalizing Human Video Pose Estimation (CVPR 2016) </br>
[[Paper](https://arxiv.org/pdf/1511.06676.pdf)][[Homepage](https://www.robots.ox.ac.uk/~vgg/data/pose/index.html)] </br>
*50 videos, 5,000 annotated frames*

* **Narrated Instruction Videos**: Unsupervised Learning from Narrated Instruction Videos </br>
[[Paper](https://www.di.ens.fr/willow/research/instructionvideos/paper.pdf)][[Homepage](https://www.di.ens.fr/willow/research/instructionvideos/)] </br>
*150 videos, 800,000 frames, five tasks: Making a coffee, Changing car tire, Performing cardiopulmonary resuscitation (CPR), Jumping a
car and Repotting a plant*

* **EPIC-KITCHENS**: Scaling Egocentric Vision: The EPIC-KITCHENS Dataset</br> 
[[Paper](https://openaccess.thecvf.com/content_ECCV_2018/papers/Dima_Damen_Scaling_Egocentric_Vision_ECCV_2018_paper.pdf)][[Homepage](https://epic-kitchens.github.io/2021)]</br>
*32 participants, 11.5M frames, 39.6K action segments, 454.3K object bounding boxes, 149 action classes, 323 object classes*

* **HOMAGE**: Home Action Genome: Cooperative Compositional Action Understanding (CVPR 2021) </br> 
[[Paper]()][[Homepage](https://homeactiongenome.org/)]</br> 
*27 participants, 12 sensor types, 75 activities, 453 atomic actions, 1,752 synchronized sequences, 86 object classes, 29 relationship classes, 497,534 bounding boxes, 583,481 relationships.*

* **MMAct**: A Large-Scale Dataset for Cross Modal Human Action Understanding (ICCV 2019) </br> 
[[Paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Kong_MMAct_A_Large-Scale_Dataset_for_Cross_Modal_Human_Action_Understanding_ICCV_2019_paper.pdf)][[Homepage](https://mmact19.github.io/2019/)]</br>
*36k video clips, 37 action classes, RGB+Keypoints+Acc+Gyo+Ori+Wi-Fi+Presure*

* **LEMMA**: A Multi-view Dataset for LEarning Multi-agent Multi-task Activities (ECCV 2020) </br> 
[[Paper](https://arxiv.org/pdf/2007.15781.pdf)][[Homepage](https://sites.google.com/view/lemma-activity)]</br>
*RGB-D, 641 action classes, 11,781 action segments, 4.6M frames*

* **Action Genome**: Actions as Compositions of Spatio-temporal Scene Graphs (CVPR 2020) </br> 
[[Paper](https://arxiv.org/pdf/1912.06992.pdf)][[Homepage](https://www.actiongenome.org/)]</br>
*10K videos, 0.4M objects, 1.7M visual relationships*

* **TITAN**: Future Forecast using Action Priors (CVPR 2020) </br> 
[[Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Malla_TITAN_Future_Forecast_Using_Action_Priors_CVPR_2020_paper.pdf)][[Homepage](https://usa.honda-ri.com/titan)]</br>
*700 labeled video-clips, 50 labels including vehicle states and actions, pedestrian age groups, and targeted pedestrian action attributes*

* **PKU-MMD**: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding (ACM Multimedia Workshop) </br> 
[[Paper](https://arxiv.org/abs/1703.07475)][[Homepage](https://github.com/ECHO960/PKU-MMD#pku-mmd-a-large-scale-benchmark-for-continuous-multi-modal-human-action-understanding)]</br>
*1,076 long video sequences, 51 action categories, performed by 66 subjects in three camera views, 20,000 action instances, 5.4 million frames, RGB+depth+Infrared Radiation+Skeleton*

* **Breakfast**: The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities </br> 
[[Paper](https://openaccess.thecvf.com/content_cvpr_2014/papers/Kuehne_The_Language_of_2014_CVPR_paper.pdf)][[Homepage](https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/)]</br>
*52 participants, 10 distinct cooking activities captured in 18 different kitchens, 48 action classes, 11,267 clips*

* **HACS**: Human Action Clips and Segments Dataset for Recognition and Temporal Localization </br>
[[Paper](https://arxiv.org/pdf/1712.09374.pdf)][[Homepage](http://hacs.csail.mit.edu/)]</br>
*HACS Clips: 1.5M annotated clips sampled from 504K untrimmed videos, HACS Segments: 139K action segments densely annotated in 50K untrimmed videos spanning 200 action categories*

* **VLOG**: From Lifestyle Vlogs to Everyday Interactions (CVPR 2018) </br>
[[Paper](https://web.eecs.umich.edu/~fouhey//2017/VLOG/paper.pdf)][[Homepage](https://web.eecs.umich.edu/~fouhey//2017/VLOG/index.html)] </br>
*114K video clips, 10.7K participants, Annotations: Hand/Semantic Object, Hand Contact State, Scene Classification*

* **YouCook2**: YouCookII Dataset </br>
[[Paper](http://youcook2.eecs.umich.edu/static/YouCookII/youcookii_readme.pdf)][[Homepage](http://youcook2.eecs.umich.edu/)] </br>
*2000 long untrimmed videos, 89 cooking recipes, each recipe includes 5 to 16 steps, each step should be described with one sentence*

* **How2**: A Large-scale Dataset for Multimodal Language Understanding (NeurIPS 2018) </br>
[[Paper](https://arxiv.org/pdf/1811.00347.pdf)][[Homepage](https://github.com/srvk/how2-dataset)] </br>
*80,000 clips, word-level time alignments to the ground-truth English subtitles*

* **COIN**: A Large-scale Dataset for Comprehensive Instructional Video Analysis (CVPR 2019) </br>
[[Paper](https://arxiv.org/pdf/1903.02874.pdf)][[Homepage](https://coin-dataset.github.io/)] </br>
*11,827 videos, 180 tasks, 12 domains, 46,354 annotated segments*

* **MovieGraphs**: Towards Understanding Human-Centric Situations from Videos (CVPR 2018) </br>
[[Paper](https://arxiv.org/pdf/1712.06761.pdf)][[Homepage](http://moviegraphs.cs.toronto.edu/)] </br>
*7,637 movie clips, 51 movies, annotations: scene, situation, description, graph (Character, Attributes, Relationship, Interaction, Topic, Reason, Time stamp)*

* **HowTo100M**: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips (ICCV 2019) </br>
[[Paper](https://arxiv.org/pdf/1906.03327.pdf)][[Homepage](https://www.di.ens.fr/willow/research/howto100m/)] </br>
*136 million video clips sourced from 1.22M narrated instructional web videos, 23k different visual tasks*

* **Oops!**: Predicting Unintentional Action in Video (CVPR 2020) </br>
[[Paper](https://arxiv.org/pdf/1911.11206.pdf)][[Homepage](https://oops.cs.columbia.edu/)] </br>
*20,338 videos, 7,368 annotated for training, 6,739 annotated for testing*

* **MovieNet**: A Holistic Dataset for Movie Understanding (ECCV 2020) </br>
[[Paper](https://arxiv.org/pdf/2007.10937.pdf)][[Homepage](http://movienet.site/)] </br>
*1,100 movies, 1.1M characters with bounding boxes and identities, 42K scene boundaries, 2.5K aligned description sentences, 65K tags of place and action, and 92
K tags of cinematic style*

* **HVU**: Large Scale Holistic Video Understanding (ECCV 2020) </br>
[[Paper](https://arxiv.org/pdf/1904.11451.pdf)][[Homepage](https://holistic-video-understanding.github.io/)] </br>
*572k videos in total with 9 million annotations for training, validation and test set spanning over 3142 labels, semantic aspects defined on categories of scenes, objects, actions, events, attributes and concepts*

* **RareAct**: A video dataset of unusual interactions </br>
[[Paper](https://arxiv.org/pdf/2008.01018.pdf)][[Homepage]()]</br>
*122 different actions, 7,607 clips, 905 videos, 19 verbs, 38 nouns*

* **Countix**: Counting Out Time: Class Agnostic Video Repetition Counting in the Wild (CVPR 2020) </br>
[[Paper](https://arxiv.org/pdf/2006.15418v1.pdf)][[Homepage](https://sites.google.com/view/repnet)] </br>
*8,757 videos*

* **EEV**: A Large-Scale Dataset for Studying Evoked Expressions from Video </br> 
[[Paper](https://arxiv.org/abs/2001.05488)][[Homepage](https://github.com/google-research-datasets/eev)]</br>
*Each video is annotated at 6 Hz with 15 continuous evoked expression labels, 36.7 million annotations of viewer facial reactions to 23,574 videos (1,700 hours)*

* **FineGym**: A Hierarchical Video Dataset for Fine-grained Action Understanding (CVPR 2020) </br>
[[Paper](https://arxiv.org/pdf/2004.06704.pdf)][[Homepage](https://sdolivia.github.io/FineGym/)]
*10 event categories, including 6 male events and 4 female events, 530 element categories*

## Group Activity Recognition
* **Volleyball**: A Hierarchical Deep Temporal Model for Group Activity Recognition
[[Paper](https://arxiv.org/pdf/1511.06040.pdf)][[Homepage](https://github.com/mostafa-saad/deep-activity-rec)] </br>
*4,830 clips, 8 group activity classes, nine individual actions*

* **Collective**: What are they doing? : Collective activity classification using spatio-temporal relationship among people
[[Paper](http://vhosts.eecs.umich.edu/vision//paper/Wongun_CollectiveActivityRecognition09.pdf)][[Homepage](http://vhosts.eecs.umich.edu/vision//activity-dataset.html)] </br>
*5 different collective activities, 44 clips* 

## 360 Videos
* **Pano2Vid**: Automatic Cinematography for Watching 360° Videos (ACCV 2016) </br>
[[Paper](http://vision.cs.utexas.edu/projects/Pano2Vid/accv2016-0327su.pdf)][[Homepage](http://vision.cs.utexas.edu/projects/Pano2Vid/)] </br>
*20 out of 86 360° videos have labels for testing; 9,171 normal videos captured by humans for inference in training; topics: Soccer, Mountain Climbing, Parade, and Hiking)*

* **Deep 360 Pilot**: Learning a Deep Agent for Piloting through 360◦ Sports Videos (CVPR 2017) </br>
[[Paper](https://arxiv.org/pdf/1705.01759.pdf)][[Homepage](https://aliensunmin.github.io/project/360video/)] </br>
*342 360° videos, topics: basketball, parkour, BMX, skateboarding, and dance*

* **YT-ALL**: Self-Supervised Generation of Spatial Audio for 360◦ Video (NeurIPS 2018)
[[Paper](https://arxiv.org/pdf/1809.02587.pdf)][[Homepage](https://pedro-morgado.github.io/spatialaudiogen/)] </br>
*1,146 videos, half of the videos are live music performances*

* **YT360**:  Learning Representations from Audio-Visual Spatial Alignment (NeurIPS 2020) </br>
[[Paper](https://papers.nips.cc/paper/2020/file/328e5d4c166bb340b314d457a208dc83-Paper.pdf)][[Homepage](https://github.com/pedro-morgado/AVSpatialAlignment)] </br>
*topics: musical performances, vlogs, sports, and others*

## Action/Event Localization
* **AVA**: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions </br> 
[[Paper](https://arxiv.org/abs/1705.08421)][[Homepage](http://research.google.com/ava/)]</br>
*80 atomic visual actions in 430 15-minute video clips, 1.58M action labels with multiple labels per person occurring frequently*

* **AVA-Kinetics**: The AVA-Kinetics Localized Human Actions Video Dataset </br> 
[[Paper](https://arxiv.org/pdf/2005.00214.pdf)][[Homepage](http://research.google.com/ava/)]</br>
*230k clips, 80 AVA action classes*

* **MUSES**: Multi-shot Temporal Event Localization: a Benchmark (CVPR 2021) </br> 
[[Paper](https://arxiv.org/pdf/2012.09434.pdf)][[Homepage](https://songbai.site/muses/)]</br>
*31,477 event instances, 716 video hours, 19 shots per instance, 176 shots per video, 25 categories, 3,697 videos*

## Actor/Action Segmentation
* **A2D**: Can Humans Fly? Action Understanding with Multiple Classes of Actors (CVPR 2015) </br>
[[Paper](https://www.cs.rochester.edu/~cxu22/p/cvpr2015_a2d_paper.pdf)][[Homepage](https://web.eecs.umich.edu/~jjcorso/r/a2d/)] </br>
*3,782 videos, actors: adult, baby, bird, cat, dog, ball and car, actions: climbing, crawling, eating, flying, jumping, rolling, running, and walking*

* **J-HMDB**: Towards understanding action recognition (ICCV 2013) </br>
[[Paper](https://hal.inria.fr/hal-00906902/document)][[Homepage](http://jhmdb.is.tue.mpg.de/)] </br>
*31,838 annotated frames, 21 categories involving a single person in action: brush hair, catch, clap, climb stairs, golf, jump, kick ball, pick, pour, pull-up, push, run, shoot ball, shoot bow, shoot gun, sit, stand, swing baseball, throw, walk, wave*

*  **A2D Sentences & J-HMDB Sentences**: Actor and Action Video Segmentation from a Sentence (CVPR 2018) </br>
[[Paper](https://openaccess.thecvf.com/content_cvpr_2018/papers/Gavrilyuk_Actor_and_Action_CVPR_2018_paper.pdf)][[Homepage](https://kgavrilyuk.github.io/publication/actor_action/)] </br>
*A2D Sentences: 6,656 sentences, including 811 different nouns, 225 verbs and 189 adjectives, J-HMDB Sentences: 928 sentences, including 158 different
nouns, 53 verbs and 23 adjectives*

## Audiovisual Learning
* **Audio Set**: An ontology and human-labeled dataset for audio events (ICASSP 2017) </br>
[[Paper](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45857.pdf)][[Homepage](https://research.google.com/audioset/)] </br>
*632 audio event classes, 2,084,320 human-labeled 10-second sound clips*

* **MUSIC**: The Sound of Pixels (ECCV 2018) </br>
[[Paper](https://arxiv.org/pdf/1804.03160.pdf)][[Homepage](http://sound-of-pixels.csail.mit.edu/)] <br>
*685 untrimmed videos, 11 instrument categories*

* **AudioSet ZSL**: Coordinated Joint Multimodal Embeddings for Generalized Audio-Visual Zero-shot Classification and Retrieval of Videos (WACV 2020) </br>
[[Paper](https://openaccess.thecvf.com/content_WACV_2020/papers/Parida_Coordinated_Joint_Multimodal_Embeddings_for_Generalized_Audio-Visual_Zero-shot_Classification_and_WACV_2020_paper.pdf)][[Homepage](https://github.com/krantiparida/AudioSetZSL)] </br>
*33 classes, 156,416 videos*

* **Kinetics-Sound**: Look, Listen and Learn (ICCV 2017) </br>
[[Paper](https://openaccess.thecvf.com/content_ICCV_2017/papers/Arandjelovic_Look_Listen_and_ICCV_2017_paper.pdf)] </br>
*34 action classes from Kinetics*

* **SoundNet**: Learning Sound Representations from Unlabeled Video (NIPS 2016) </br>
[[Paper](https://arxiv.org/pdf/1610.09001.pdf)][[Homepage](http://soundnet.csail.mit.edu/)]</br>
*2+ million videos*

* **AVE**: Audio-Visual Event Localization in Unconstrained Videos (ECCV 2018) </br> 
[[Paper](https://openaccess.thecvf.com/content_ECCV_2018/papers/Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper.pdf)][[Homepage](https://sites.google.com/view/audiovisualresearch)]</br>
*4,143 10-second videos, 28 audio-visual events*

* **LLP**: Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video Parsing (ECCV 2020) </br> 
[[Paper](https://arxiv.org/pdf/2007.10558.pdf)][[Homepage](https://github.com/YapengTian/AVVP-ECCV20)]</br>
*11,849 YouTube video clips, 25 event categories*

* **VGG-Sound**: A large scale audio-visual dataset </br> 
[[Paper](https://arxiv.org/abs/2004.14368)][[Homepage](https://www.robots.ox.ac.uk/~vgg/data/vggsound/)]</br>
*200k videos, 309 audio classes*

* **YouTube-ASMR-300K**: Telling Left from Right: Learning Spatial Correspondence of Sight and Sound (CVPR 2020) </br>
[[Paper](https://arxiv.org/pdf/2006.06175.pdf)][[Homepage](https://karreny.github.io/telling-left-from-right/)] </br>
*300K 10-second video clips with spatial audio*

* **XD-Violence**: Not only Look, but also Listen: Learning Multimodal Violence Detection under Weak Supervision (ECCV 2020) </br>
[[Paper](https://roc-ng.github.io/XD-Violence/images/paper.pdf)][[Homepage](https://roc-ng.github.io/XD-Violence/)] </br>
* 4754 untrimmed videos*

* **VGG-SS**: Localizing Visual Sounds the Hard Way (CVPR 2021)</br> 
[[Paper](https://arxiv.org/pdf/2104.02691.pdf)][[Homepage](https://www.robots.ox.ac.uk/~vgg/research/lvs/)] </br>
*5K videos, 200 categories*

* **VoxCeleb**: Large-scale speaker verification in the wild </br>
[[Paper](https://www.robots.ox.ac.uk/~vgg/publications/2019/Nagrani19/nagrani19.pdf)][[Homepage](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/)] </br>
*a million ‘real-world’ utterances, over 7000 speakers*

* **EmoVoxCeleb**: Emotion Recognition in Speech using Cross-Modal Transfer in the Wild </br>
[[Paper](https://www.robots.ox.ac.uk/~vgg/publications/2018/Albanie18/albanie18.pdf)][[Homepage](https://www.robots.ox.ac.uk/~vgg/research/cross-modal-emotions/)]</br>
*1,251 speakers*

* **Speech2Gesture**: Learning Individual Styles of Conversational Gesture (CVPR 2019) </br>
[[Paper](https://arxiv.org/pdf/1906.04160.pdf)][[Homepage](http://people.eecs.berkeley.edu/~shiry/projects/speech2gesture/)] </br>
*144-hour person-specific video, 10 speakers*

* **AVSpeech**: Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation </br>
[[Paper](https://arxiv.org/pdf/1804.03619.pdf)][[Homepage](https://looking-to-listen.github.io/avspeech/)]</br>
*150,000 distinct speakers, 290k YouTube videos*

* **LRW**: Lip Reading in the Wild (ACCV 2016) </br>
[[Paper](https://www.robots.ox.ac.uk/~vgg/publications/2016/Chung16/chung16.pdf)][[Homepage](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html)] </br>
*1000 utterances of 500 different words*

* **Countix-AV & Extreme Countix-AV**: Repetitive Activity Counting by Sight and Sound (CVPR 2021) </br>
[[Paper](https://arxiv.org/pdf/2103.13096.pdf)][[Homepage](https://github.com/xiaobai1217/RepetitionCounting)] </br>
*1,863 videos in Countix-AV, 214 videos in Extreme Countix-AV*

## Others
* **QUVA Repetition**: Real-World Repetition Estimation by Div, Grad and Curl (CVPR 2018) </br>
[[Paper](https://openaccess.thecvf.com/content_cvpr_2018/papers/Runia_Real-World_Repetition_Estimation_CVPR_2018_paper.pdf)][[Homepage](http://tomrunia.github.io/projects/repetition/)] </br>
*100 videos*

* **Real-world Flag & FlagSim**: Cloth in the Wind: A Case Study of Physical Measurement through Simulation (CVPR 2020) </br>
[[Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Runia_Cloth_in_the_Wind_A_Case_Study_of_Physical_Measurement_CVPR_2020_paper.pdf)][[Homepage](http://tomrunia.github.io/projects/cloth/)] </br>
*Real-world Flag: 2.7K train and 1.3K videos clips, FlagSim: 1,000 mesh sequences, 14, 000 training examples*
