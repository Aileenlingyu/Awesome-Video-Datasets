# Awesome-Video-Datasets

## Action Recognition

* <ins>**[HMDB](https://serre-lab.clps.brown.edu/wp-content/uploads/2012/08/Kuehne_etal_iccv11.pdf)**: A Large Video Database for Human Motion Recognition</ins> (ICCV 2011) [Homepage](https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/)

* <ins>**[UCF101](https://www.crcv.ucf.edu/papers/UCF101_CRCV-TR-12-01.pdf)**: A Dataset of 101 Human Actions Classes From Videos in The Wild</ins> [Homepage](https://www.crcv.ucf.edu/data/UCF101.php)

* <ins>**[ActivityNet](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Heilbron_ActivityNet_A_Large-Scale_2015_CVPR_paper.pdf)**: A Large-Scale Video Benchmark for Human Activity Understanding</ins> (CVPR 2015)[Homepage](http://activity-net.org/index.html)

* **Kinetics**: [Kinetics-400](https://arxiv.org/abs/1705.06950) [Kinetics-600](https://arxiv.org/abs/1808.01340) [Kinetics-700](https://arxiv.org/abs/1907.06987) [Homepage](https://deepmind.com/research/open-source/kinetics)

* <ins>**[Charades](http://ai2-website.s3.amazonaws.com/publications/hollywood-homes.pdf)**: Hollywood in Homes: Crowdsourcing Data
Collection for Activity Understanding</ins> (ECCV 2016) [Homepage](https://prior.allenai.org/projects/charades)

* <ins>**[Charades-Ego](https://arxiv.org/pdf/1804.09627.pdf)**: Actor and Observer: Joint Modeling of First and Third-Person Videos</ins> (CVPR 2018) [Homepage](https://prior.allenai.org/projects/charades-ego)

* <ins>**[Moments in Time Dataset](http://moments.csail.mit.edu/TPAMI.2019.2901464.pdf)**: one million videos for event understanding</ins> (TPAMI 2019) [Homepage](http://moments.csail.mit.edu/)

* <ins>**[Multi-Moments in Time](https://arxiv.org/pdf/1911.00232.pdf)**: Learning and Interpreting Models for Multi-Action Video Understanding</ins> [Homepage](http://moments.csail.mit.edu/)

* <ins>**[EPIC-KITCHENS](https://openaccess.thecvf.com/content_ECCV_2018/papers/Dima_Damen_Scaling_Egocentric_Vision_ECCV_2018_paper.pdf)**: Scaling Egocentric Vision: The EPIC-KITCHENS Dataset</ins> [Homepage](https://epic-kitchens.github.io/2021)

* <ins>**HOMAGE**: Home Action Genome: Cooperative Compositional Action Understanding</ins> (CVPR 2021) [Homepage](https://homeactiongenome.org/)</br> 
*27 participants, 12 sensor types, 75 activities, 453 atomic actions, 1,752 synchronized sequences, 86 object classes, 29 relationship classes, 497,534 bounding boxes, 583,481 relationships.*

* <ins>**[MMAct](https://openaccess.thecvf.com/content_ICCV_2019/papers/Kong_MMAct_A_Large-Scale_Dataset_for_Cross_Modal_Human_Action_Understanding_ICCV_2019_paper.pdf)**: A Large-Scale Dataset for Cross Modal Human Action Understanding</ins> (ICCV 2019) [Homepage](https://mmact19.github.io/2019/)

* <ins>**[LEMMA](https://arxiv.org/pdf/2007.15781.pdf)**: A Multi-view Dataset for LEarning Multi-agent Multi-task Activities</ins> (ECCV 2020) [Homepage](https://sites.google.com/view/lemma-activity)

* <ins>**[Action Genome](https://arxiv.org/pdf/1912.06992.pdf)**: Actions as Compositions of Spatio-temporal Scene Graphs</ins> (CVPR 2020) [Homepage](https://www.actiongenome.org/)

* <ins>**[TITAN](https://openaccess.thecvf.com/content_CVPR_2020/papers/Malla_TITAN_Future_Forecast_Using_Action_Priors_CVPR_2020_paper.pdf)**: Future Forecast using Action Priors</ins> (CVPR 2020) [Homepage](https://usa.honda-ri.com/titan)

* <ins>**[PKU-MMD](https://arxiv.org/abs/1703.07475)**: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding</ins> (ACM Multimedia Workshop) [Homepage](https://github.com/ECHO960/PKU-MMD#pku-mmd-a-large-scale-benchmark-for-continuous-multi-modal-human-action-understanding)

* <ins>**[Breakfast](https://openaccess.thecvf.com/content_cvpr_2014/papers/Kuehne_The_Language_of_2014_CVPR_paper.pdf)**: The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities</ins> [Homepage](https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/)

## Action/Event Localization
* **[AVA](https://arxiv.org/abs/1705.08421)**: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions [Homepage](http://research.google.com/ava/)
* **[EEV](https://arxiv.org/abs/2001.05488)**: A Large-Scale Dataset for Studying Evoked Expressions from Video [Homepage](https://github.com/google-research-datasets/eev)
* **[MUSES](https://arxiv.org/pdf/2012.09434.pdf)**: Multi-shot Temporal Event Localization: a Benchmark (CVPR 2021) [Homepage](https://songbai.site/muses/)

## Audiovisual Learning
* **[AVE](https://openaccess.thecvf.com/content_ECCV_2018/papers/Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper.pdf)**: Audio-Visual Event Localization in Unconstrained Videos [Homepage](https://sites.google.com/view/audiovisualresearch)
* **[LLP](https://arxiv.org/pdf/2007.10558.pdf)**: Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video Parsing [Homepage](https://github.com/YapengTian/AVVP-ECCV20)
* **[VGG-Sound](https://arxiv.org/abs/2004.14368)**: A large scale audio-visual dataset [Homepage](https://www.robots.ox.ac.uk/~vgg/data/vggsound/)
* **[VGG-SS](https://arxiv.org/pdf/2104.02691.pdf)**: Localizing Visual Sounds the Hard Way (CVPR 2021) [Homepage](https://www.robots.ox.ac.uk/~vgg/research/lvs/)
